# MLAssigment01
Linear Regression Models – Comprehensive Study
Objective

The objective of this lab assignment is to implement, analyze, and compare different Linear Regression models including:

Simple Linear Regression

Multiple Linear Regression

Polynomial Regression

Regularization Techniques (Ridge and Lasso)

This experiment helps in understanding model behavior, evaluation metrics, assumptions, and performance comparison using real-world data.

Tools Used

Python

Jupyter Notebook / Google Colab

Libraries: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn

Dataset Description

The dataset used in this assignment contains:

At least three input features

One continuous target variable

The dataset was loaded, cleaned, and analyzed before applying regression models.

PART A: EXPLORATORY DATA ANALYSIS
In this section:

Loaded the dataset and displayed basic information

Computed summary statistics

Checked for missing values

Visualized feature distributions

Generated a correlation heatmap

EDA helped in understanding relationships between features and the target variable.

Part B – Simple Linear Regression

Selected one independent feature

Built a Simple Linear Regression model

Plotted the regression line

Interpreted slope and intercept

This model helped in understanding the direct relationship between a single feature and the target.

 Part C – Multiple Linear Regression

Used multiple input features

Trained a Multiple Linear Regression model

Evaluated performance using:

Mean Squared Error (MSE)

Root Mean Squared Error (RMSE)

R² Score

The coefficients were interpreted to understand feature importance.

Part D – Polynomial Regression

Applied Polynomial Features to capture non-linear relationships

Compared Linear vs Polynomial model performance

Observed change in RMSE and R²

This helped in understanding model flexibility and overfitting behavior.

 Part E – Regularization (Ridge & Lasso)

Implemented Ridge Regression

Implemented Lasso Regression

Compared coefficient shrinkage

Observed feature selection effect in Lasso

Regularization helped reduce overfitting and improve generalization.

 Part F – Model Diagnostics

Plotted residuals vs predicted values

Verified regression assumptions:

Linearity

Homoscedasticity

Independence

Normality of residuals

 Evaluation Metrics Used

MSE (Mean Squared Error)

RMSE (Root Mean Squared Error)

R² Score

These metrics were used to compare model performance.


Observations

Multiple Linear Regression performed better than Simple Linear Regression.

Polynomial Regression improved performance for non-linear patterns but may risk overfitting.

Ridge and Lasso helped control large coefficients.

Lasso demonstrated feature selection ability.

 Conclusion

Through this assignment, I gained practical understanding of:

End-to-end regression workflow

Data preprocessing and visualization

Model building and evaluation

Bias-variance tradeoff

Regularization techniques

This lab also helped me learn GitHub repository management and professional documentation practices.
