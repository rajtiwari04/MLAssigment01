# MLAssigment01
Linear Regression Models â€“ Comprehensive Study
Objective

The objective of this lab assignment is to implement, analyze, and compare different Linear Regression models including:

Simple Linear Regression

Multiple Linear Regression

Polynomial Regression

Regularization Techniques (Ridge and Lasso)

This experiment helps in understanding model behavior, evaluation metrics, assumptions, and performance comparison using real-world data.

ðŸ›  Tools Used

Python

Jupyter Notebook / Google Colab

Libraries: NumPy, Pandas, Matplotlib, Seaborn, Scikit-learn

ðŸ“‚ Dataset Description

The dataset used in this assignment contains:

At least three input features

One continuous target variable

The dataset was loaded, cleaned, and analyzed before applying regression models.

Part A â€“ Exploratory Data Analysis (EDA)

In this section:

Loaded the dataset and displayed basic information

Computed summary statistics

Checked for missing values

Visualized feature distributions

Generated a correlation heatmap

EDA helped in understanding relationships between features and the target variable.

Part B â€“ Simple Linear Regression

Selected one independent feature

Built a Simple Linear Regression model

Plotted the regression line

Interpreted slope and intercept

This model helped in understanding the direct relationship between a single feature and the target.

 Part C â€“ Multiple Linear Regression

Used multiple input features

Trained a Multiple Linear Regression model

Evaluated performance using:

Mean Squared Error (MSE)

Root Mean Squared Error (RMSE)

RÂ² Score

The coefficients were interpreted to understand feature importance.

Part D â€“ Polynomial Regression

Applied Polynomial Features to capture non-linear relationships

Compared Linear vs Polynomial model performance

Observed change in RMSE and RÂ²

This helped in understanding model flexibility and overfitting behavior.

 Part E â€“ Regularization (Ridge & Lasso)

Implemented Ridge Regression

Implemented Lasso Regression

Compared coefficient shrinkage

Observed feature selection effect in Lasso

Regularization helped reduce overfitting and improve generalization.

 Part F â€“ Model Diagnostics

Plotted residuals vs predicted values

Verified regression assumptions:

Linearity

Homoscedasticity

Independence

Normality of residuals

 Evaluation Metrics Used

MSE (Mean Squared Error)

RMSE (Root Mean Squared Error)

RÂ² Score

These metrics were used to compare model performance.


Observations

Multiple Linear Regression performed better than Simple Linear Regression.

Polynomial Regression improved performance for non-linear patterns but may risk overfitting.

Ridge and Lasso helped control large coefficients.

Lasso demonstrated feature selection ability.

 Conclusion

Through this assignment, I gained practical understanding of:

End-to-end regression workflow

Data preprocessing and visualization

Model building and evaluation

Bias-variance tradeoff

Regularization techniques

This lab also helped me learn GitHub repository management and professional documentation practices.
